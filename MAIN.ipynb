{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc026e01",
   "metadata": {},
   "source": [
    "# Packages installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b04d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install packages needed \n",
    "!pip install matplotlib\n",
    "!pip install particles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbbe83",
   "metadata": {},
   "source": [
    "# Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9fb6f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "#import scipy.stats as stats\n",
    "\n",
    "# Modules from particles\n",
    "import particles \n",
    "from particles import distributions as dists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6230a",
   "metadata": {},
   "source": [
    "# A word about what we are trying to achieve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37081cee",
   "metadata": {},
   "source": [
    "Generally, for the type of problems we are speaking about, we deal with:\n",
    "- An hidden variable $X_t$ that follows a markov process. This variable can be multivariate. It is characterized by a **transition kernel**.\n",
    "- An observed variable $Y_t$ whose distribution depends on $X_t$. It is characterized by an **emission law**. \n",
    "\n",
    "The basic task is, given that we *know* the transition kernel, the emission law, and all their parameters, to recover the $X_t$ based on a sequence of $Y_t$ (*filtering* / *complete smoothing*).\n",
    "\n",
    "A more challening tasks is to perform **bayesian inference**, that is, to estimate the posterior of the parameters given the data (and prior on the parameters). This task relies on particle MCMC algorithms. \n",
    "\n",
    "In our setting: \n",
    "\n",
    "- $X$ is composed of \n",
    "    - $u_t$, the expression level. It is the variable that, in real life, researchers are trying to recover. \n",
    "    - $s_t$, the local scaling term. A variable that follows a markov process independently from everything else and will be useful at some point. \n",
    "    - We might even add $a_t$ and $x_t$\n",
    "- $Y$ is simply $y_t$, the read counts. It is basically **a noisy observation of $u_t$**. Its distribution is described by the **emission law** that involves $u_t$ and $s_t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35cbf9a",
   "metadata": {},
   "source": [
    "## What we have to do \n",
    "\n",
    "Given the level of complexity of the model, our assignment has changed. We can focus on:\n",
    "\n",
    "1. Creating the model\n",
    "    - Having a working subclass of Feynman Kac \n",
    "    - being able to generate data (should not be too hard)\n",
    "\n",
    "2. Using the bootstrap filter\n",
    "\n",
    "3. Using the PMMH algorithm to perform some bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fccebdc",
   "metadata": {},
   "source": [
    "## How we are going to do it \n",
    "\n",
    "Since our model is pretty complicated, the previous approach subclassing (ssm.StateSpaceModel) will not work. \n",
    "Instead, we are going to define ourselves a Feynman-Kac model as explained here: \n",
    "https://particles-sequential-monte-carlo-in-python.readthedocs.io/en/latest/notebooks/Defining_Feynman-Kac_models_manually.html\n",
    "\n",
    "- In the method M, we are going to define a way to sample from our X\n",
    "    - If we do it smartly, we should be able to do it using a function that we could also use to generate the data \n",
    "- In the method logG, we are going to compute the loglikelihood \n",
    "\n",
    "!Note that xp and x are tables containing N particle (we will have to loop throught them)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19294a44",
   "metadata": {},
   "source": [
    "## Various notes and tips\n",
    "\n",
    "- We can further simplify the model if it is too complicated\n",
    "- We could use **numba** (a package) to make the loops run faster\n",
    "- Compute the loglikelihood, not the likelihood. When we have to sum likelihood, it is always possible to use the log_sum_exp trick\n",
    "- Simulate the data using parameter's value that make sens (look in the articles) or go looking for a datasource (the Parseq module might have some)\n",
    "- For the initial law, find it in the paper or use something that makes sens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bc3928",
   "metadata": {},
   "source": [
    "## Choice regarding the drifts\n",
    "The explanations in the supplementary materials are unclear. What we are chosing to do is:\n",
    "\n",
    "**Upward drift**\n",
    "\n",
    "$u_{t+1} = u_t + Z, \\quad Z \\sim \\mathcal{E}(\\frac{\\lambda_u}{u_t})$\n",
    "\n",
    "**Downward drift**\n",
    "\n",
    "$u_{t+1} = u_t - Z, \\quad Z \\sim \\mathcal{E}(\\frac{\\lambda_d}{u_t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed3ae72",
   "metadata": {},
   "source": [
    "## Regarding the emission law\n",
    "\n",
    "**We have two definitions of it**\n",
    "- The 1st will be useful to generate the data \n",
    "- The 2nd will be useful to compute the likelihood (we can truncate the sum at 10 or 30)\n",
    "\n",
    "**Other tips**\n",
    "- Likelihod of a truncated negative binomial: compute it the usual way, then divide by (1 - probability of 0)\n",
    "- Poisson law truncated in 0: keep generating until you gave a non-zero value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb338c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The example from the package \n",
    "\n",
    "class GaussianProb(particles.FeynmanKac):\n",
    "    def __init__(self, a=0., b=1., T=10):\n",
    "        self.a, self.b, self.T = a, b, T\n",
    "\n",
    "    def M0(self, N):\n",
    "        return stats.norm.rvs(size=N)\n",
    "\n",
    "    def M(self, t, xp):\n",
    "        return stats.norm.rvs(loc=xp, size=xp.shape)\n",
    "\n",
    "    def logG(self, t, xp, x):\n",
    "        return np.where((x < self.b) & (x > self.a), 0., -np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968dd428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majX(xp, eta, alpha, zeta, beta, beta_0, gamma_u, gamma_d, lambda_u, lambda_d,\n",
    "            alpha_s, kappa_s):\n",
    "    \n",
    "    x = np.zeros(shape = (2))\n",
    "    # Computations for u_t\n",
    "    uniu = dists.Uniform(a = 0., b = 1.).rvs(size = 1) # Uniform to \"choose a move\"\n",
    "        \n",
    "    if xp[0] == 0 :\n",
    "        if uniu <= eta:\n",
    "            # Dirac on xp = 0 \n",
    "            x[0] = 0\n",
    "        else:\n",
    "            # Exponential law with rate zeta \n",
    "            x[0] = dists.Gamma(1, zeta).rvs(size = 1)\n",
    "            \n",
    "    elif xp[0] !=0:\n",
    "        if uniu <= alpha:\n",
    "            # Dirac on xp\n",
    "            x[0] = xp[0]\n",
    "\n",
    "        elif uniu <= alpha + beta:\n",
    "            # Exponential law with rate zeta \n",
    "            x[0] = dists.Gamma(1, zeta).rvs(size = 1)\n",
    "\n",
    "        elif uniu <= alpha + beta + beta_0:\n",
    "            # Dirac on 0\n",
    "            x[0] = 0\n",
    "\n",
    "        elif uniu <= alpha + beta + beta_0 + gamma_u:\n",
    "            # Drift upward\n",
    "            x[0] = xp[0] + dists.Gamma(1, lambda_u/xp[0]).rvs(size = 1)\n",
    "\n",
    "        else:\n",
    "            # Drift downward \n",
    "            x[0] = xp[0] - dists.Gamma(1, self.lambda_d/xp[0]).rvs(size = 1)\n",
    "        \n",
    "        \n",
    "        # Computations for s_t\n",
    "        unis = dists.Uniform(a = 0., b = 1.).rvs(size = 1) # Uniform to \"choose\" a move \n",
    "               \n",
    "        if unis <= alpha_s:\n",
    "            x[1] = xp[1]\n",
    "        else:\n",
    "            x[1] = dists.Gamma(a = kappa_s, b = kappa_s).rvs(size = 1))\n",
    "            \n",
    "        \n",
    "        return x \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "820e8d1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2551365168.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\leo\\AppData\\Local\\Temp\\ipykernel_12824\\2551365168.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def RnaProb(particles.FeynmanKac):\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class RnaProb(particles.FeynmanKac):\n",
    "    \n",
    "    \n",
    "    def __init__(self, a = 0.): # ALl parameters should be added there\n",
    "        self.a = a # And so on for all parameters \n",
    "                 \n",
    "    def M0(self, N):\n",
    "    \n",
    "    def M(self, t, xp):\n",
    "                 \n",
    "        # Initialize empty vector\n",
    "        # It has two columns, one for u_t and one for s_t\n",
    "        x = np.zeros(shape = (N, 2)) # Maybe this N won't work\n",
    "        \n",
    "        # Loop over the particles\n",
    "        for i in range(N):\n",
    "            # Call the function that does one move using the kernel of x\n",
    "            x[i,:] = majX(xp[i,:], \n",
    "                          eta, alpha, zeta, beta, beta_0, gamma_u, gamma_d, lambda_u, lambda_d,\n",
    "                          alpha_s, kappa_s)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "                 \n",
    "    def logG(self, t, xp, x):\n",
    "                 \n",
    "                 \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b996877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.zeros(shape = (2))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bce30b",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d9373",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Should probably be transformed into a function so that it can be reused for the sampling\n",
    "                 \n",
    "            # Computations for u_t\n",
    "            uniu = dists.Uniform(a = 0., b = 1.).rvs(size = 1) # Uniform to \"choose a move\"\n",
    "        \n",
    "            if xp[i, 0] == 0 :\n",
    "                if uniu <= self.eta:\n",
    "                # Dirac on xp = 0 \n",
    "                    x[i,0] = 0\n",
    "                else:\n",
    "                    # Exponential law with rate zeta \n",
    "                    x[i,0] = dists.Gamma(1, self.zeta).rvs(size = 1)\n",
    "            \n",
    "            elif xp[i,0] !=0:\n",
    "                if uniu <= self.alpha:\n",
    "                # Dirac on xp\n",
    "                    x[i,0] = xp[i, 0]\n",
    "\n",
    "                elif uniu <= self.alpha + self.beta:\n",
    "                    # Exponential law with rate zeta \n",
    "                    x[i,0] = dists.Gamma(1, self.zeta).rvs(size = 1)\n",
    "\n",
    "                elif uniu <= self.alpha + self.beta + self.beta_0:\n",
    "                    # Dirac on 0\n",
    "                    x[i,0] = 0\n",
    "\n",
    "                elif uniu <= self.alpha + self.beta + self.beta_0 + self.gamma_u:\n",
    "                    # Drift upward\n",
    "                    x[i,0] = xp[i,0] + dists.Gamma(1, self.lambda_u/xp[i,0]).rvs(size = 1)\n",
    "\n",
    "                else:\n",
    "                    # Drift downward \n",
    "                    x[i,0] = xp[i,0] - dists.Gamma(1, self.lambda_d/xp[i,0]).rvs(size = 1)\n",
    "            \n",
    "                # Computations for s_t\n",
    "                unis = dists.Uniform(a = 0., b = 1.).rvs(size = 1) # Uniform to \"choose\" a move \n",
    "               \n",
    "                if unis <= self.alpha_s:\n",
    "                    x[i,1] = xp[i,1]\n",
    "                else:\n",
    "                    x[i,1] = dists.Gamma(a = self.kappa_s, b = self.kappa_s).rvs(size = 1)\n",
    "                 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
