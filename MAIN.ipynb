{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc026e01",
   "metadata": {},
   "source": [
    "# Packages installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b04d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install packages needed \n",
    "!pip install matplotlib\n",
    "!pip install particles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbbe83",
   "metadata": {},
   "source": [
    "# Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fb6f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Modules from particles\n",
    "import particles \n",
    "from particles import distributions as dists # Where proba distributions are defined\n",
    "from particles import state_space_models as ssm # Where state-space-models are defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6230a",
   "metadata": {},
   "source": [
    "# A word about what we are trying to achieve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37081cee",
   "metadata": {},
   "source": [
    "Generally, for the type of problems we are speaking about, we deal with:\n",
    "- An hidden variable $X_t$ that follows a markov process. This variable can be multivariate. It is characterized by a **transition kernel**.\n",
    "- An observed variable $Y_t$ whose distribution depends on $X_t$. It is characterized by an **emission law**. \n",
    "\n",
    "The basic task is, given that we *know* the transition kernel, the emission law, and all their parameters, to recover the $X_t$ based on a sequence of $Y_t$ (*filtering* / *complete smoothing*).\n",
    "\n",
    "A more challening tasks is to perform **bayesian inference**, that is, to estimate the posterior of the parameters given the data (and prior on the parameters). This task relies on particle MCMC algorithms. \n",
    "\n",
    "In our setting: \n",
    "\n",
    "- $X$ is composed of \n",
    "    - $u_t$, the expression level. It is the variable that, in real life, researchers are trying to recover. \n",
    "    - $s_t$, the local scaling term. A variable that follows a markov process independently from everything else and will be useful at some point. \n",
    "    - We might even add $a_t$ and $x_t$\n",
    "- $Y$ is simply $y_t$, the read counts. It is basically **a noisy observation of $u_t$**. Its distribution is described by the **emission law** that involves $u_t$ and $s_t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35cbf9a",
   "metadata": {},
   "source": [
    "## What we have to do \n",
    "\n",
    "Given the level of complexity of the model, our assignment has changed. We can focus on:\n",
    "\n",
    "1. Creating the model\n",
    "    - Having a working subclass of Feynman Kac \n",
    "    - being able to generate data (should not be too hard)\n",
    "\n",
    "2. Using the bootstrap filter\n",
    "\n",
    "3. Using the PMMH algorithm to perform some bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c45e991",
   "metadata": {},
   "source": [
    "## How we are going to do it \n",
    "\n",
    "Since our model is pretty complicated, the previous approach subclassing (ssm.StateSpaceModel) will not work. \n",
    "Instead, we are going to define ourselves a Feynman-Kac model as explained here: \n",
    "https://particles-sequential-monte-carlo-in-python.readthedocs.io/en/latest/notebooks/Defining_Feynman-Kac_models_manually.html\n",
    "\n",
    "- In the method M, we are going to define a way to sample from our X\n",
    "    - If we do it smartly, we should be able to do it using a function that we could also use to generate the data \n",
    "- In the method logG, we are going to compute the loglikelihood \n",
    "\n",
    "!Note that xp and x are tables containing N particle (we will have to loop throught them)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14282e",
   "metadata": {},
   "source": [
    "## Various notes and tips\n",
    "\n",
    "- We can further simplify the model if it is too complicated\n",
    "- We could use **numba** (a package) to make the loops run faster\n",
    "- Compute the loglikelihood, not the likelihood. When we have to sum likelihood, it is always possible to use the log_sum_exp trick\n",
    "- Simulate the data using parameter's value that make sens (look in the articles) or ev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9edc099",
   "metadata": {},
   "source": [
    "## Choice regarding the drifts\n",
    "The explanations in the supplementary materials are unclear. What we are chosing to do is:\n",
    "\n",
    "**Upward drift**\n",
    "\n",
    "$u_{t+1} = u_t + Z, \\quad Z \\sim \\mathcal{E}(\\frac{\\lambda_u}{u_t})$\n",
    "\n",
    "**Downward drift**\n",
    "\n",
    "$u_{t+1} = u_t - Z, \\quad Z \\sim \\mathcal{E}(\\frac{\\lambda_d}{u_t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd4eea0",
   "metadata": {},
   "source": [
    "## Regarding the emission law\n",
    "\n",
    "**We have two definitions of it**\n",
    "- The 1st will be useful to generate the data \n",
    "- The 2nd will be useful to compute the likelihood (we can truncate the sum at 10 or 30)\n",
    "\n",
    "**Other tips**\n",
    "- Likelihod of a truncated negative binomial: compute it the usual way, then divide by (1 - probability of 0)\n",
    "- Poisson law truncated in 0: keep generating until you gave a non-zero value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
