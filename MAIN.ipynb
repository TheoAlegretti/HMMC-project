{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc026e01",
   "metadata": {},
   "source": [
    "# Packages installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b04d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install packages needed \n",
    "!pip install matplotlib\n",
    "!pip install particles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbbe83",
   "metadata": {},
   "source": [
    "# Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9fb6f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "#import scipy.stats as stats\n",
    "\n",
    "# Modules from particles\n",
    "import particles \n",
    "from particles import distributions as dists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6230a",
   "metadata": {},
   "source": [
    "# A word about what we are trying to achieve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37081cee",
   "metadata": {},
   "source": [
    "Generally, for the type of problems we are speaking about, we deal with:\n",
    "- An hidden variable $X_t$ that follows a markov process. This variable can be multivariate. It is characterized by a **transition kernel**.\n",
    "- An observed variable $Y_t$ whose distribution depends on $X_t$. It is characterized by an **emission law**. \n",
    "\n",
    "The basic task is, given that we *know* the transition kernel, the emission law, and all their parameters, to recover the $X_t$ based on a sequence of $Y_t$ (*filtering* / *complete smoothing*).\n",
    "\n",
    "A more challening tasks is to perform **bayesian inference**, that is, to estimate the posterior of the parameters given the data (and prior on the parameters). This task relies on particle MCMC algorithms. \n",
    "\n",
    "In our setting: \n",
    "\n",
    "- $X$ is composed of \n",
    "    - $u_t$, the expression level. It is the variable that, in real life, researchers are trying to recover. \n",
    "    - $s_t$, the local scaling term. A variable that follows a markov process independently from everything else and will be useful at some point. \n",
    "    - We might even add $a_t$ and $x_t$\n",
    "- $Y$ is simply $y_t$, the read counts. It is basically **a noisy observation of $u_t$**. Its distribution is described by the **emission law** that involves $u_t$ and $s_t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35cbf9a",
   "metadata": {},
   "source": [
    "## What we have to do \n",
    "\n",
    "Given the level of complexity of the model, our assignment has changed. We can focus on:\n",
    "\n",
    "1. Creating the model\n",
    "    - Having a working subclass of Feynman Kac \n",
    "    - being able to generate data (should not be too hard)\n",
    "\n",
    "2. Using the bootstrap filter\n",
    "\n",
    "3. Using the PMMH algorithm to perform some bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fccebdc",
   "metadata": {},
   "source": [
    "## How we are going to do it \n",
    "\n",
    "Since our model is pretty complicated, the previous approach subclassing (ssm.StateSpaceModel) will not work. \n",
    "Instead, we are going to define ourselves a Feynman-Kac model as explained here: \n",
    "https://particles-sequential-monte-carlo-in-python.readthedocs.io/en/latest/notebooks/Defining_Feynman-Kac_models_manually.html\n",
    "\n",
    "- In the method M, we are going to define a way to sample from our X\n",
    "    - And we use a function to do so, in order to use it as well when generating the data\n",
    "- In the method logG, we are going to compute the loglikelihood \n",
    "\n",
    "!Note that xp and x are tables containing N particle (we will have to loop throught them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb338c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simple example of a feynman-kac model from the package\n",
    "\n",
    "class GaussianProb(particles.FeynmanKac):\n",
    "    def __init__(self, a=0., b=1., T=10):\n",
    "        self.a, self.b, self.T = a, b, T\n",
    "\n",
    "    def M0(self, N):\n",
    "        return stats.norm.rvs(size=N)\n",
    "\n",
    "    def M(self, t, xp):\n",
    "        return stats.norm.rvs(loc=xp, size=xp.shape)\n",
    "\n",
    "    def logG(self, t, xp, x):\n",
    "        return np.where((x < self.b) & (x > self.a), 0., -np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19294a44",
   "metadata": {},
   "source": [
    "## Various notes and tips\n",
    "\n",
    "- We can further simplify the model if it is too complicated\n",
    "- We could use **numba** (a package) to make the loops run faster\n",
    "- Compute the loglikelihood, not the likelihood. When we have to sum likelihood, it is always possible to use the log_sum_exp trick\n",
    "- Simulate the data using parameter's value that make sens (look in the articles) or go looking for a datasource (the Parseq module might have some)\n",
    "- For the initial law, find it in the paper or use something that makes sens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bc3928",
   "metadata": {},
   "source": [
    "## Choice regarding the drifts\n",
    "The explanations in the supplementary materials are unclear. What we are chosing to do is:\n",
    "\n",
    "**Upward drift**\n",
    "\n",
    "$u_{t+1} = u_t + Z, \\quad Z \\sim \\mathcal{E}(\\frac{\\lambda_u}{u_t})$\n",
    "\n",
    "**Downward drift**\n",
    "\n",
    "$u_{t+1} = u_t - Z, \\quad Z \\sim \\mathcal{E}(\\frac{\\lambda_d}{u_t})$\n",
    "\n",
    "**In both cases, the exponential is parametrized by its RATE** (whereas the numpy funciton works in terms of scale)\n",
    "\n",
    "**Be wary of the parametrization of the gamma distribution !**\n",
    "- The gamma class from particles.dists uses a RATE parameter b (a SCALE parameter 1/b)\n",
    "- In the supplementary materials, they are using a SCALE theta !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed3ae72",
   "metadata": {},
   "source": [
    "## Regarding the emission law\n",
    "\n",
    "**We have two definitions of it**\n",
    "- The 1st will be useful to generate the data \n",
    "- The 2nd will be useful to compute the likelihood (we can truncate the sum at 10 or 30)\n",
    "\n",
    "**Other tips**\n",
    "- Likelihod of a truncated negative binomial: compute it the usual way, then divide by (1 - probability of 0)\n",
    "- Poisson law truncated in 0: keep generating until you gave a non-zero value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e754ed",
   "metadata": {},
   "source": [
    "<h1 align = \"center\"> TO DO </h1>\n",
    "\n",
    "### Make a choice regarding the initial law \n",
    "Both for generating the data and for M0 in the Feynman-Kac model.\n",
    "It could make sense to use [0,0] as an itial law. Indeed:\n",
    "- For u_t, it simply means we start in a non-expressed region\n",
    "- For s_t, given it follows a gamma with probability 1/2, starting at 0 is not really an issue\n",
    "\n",
    "### Find values of the parameter that \"make sens\"\n",
    "See table S2 in the supplementary materials (p17). This is to ensure that the data generated are not totally stupid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6990b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968dd428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepX(xp, eta, alpha, zeta, beta, beta_0, gamma_u, gamma_d, lambda_u, lambda_d,\n",
    "            alpha_s, kappa_s):\n",
    "    \"\"\"\n",
    "    Performs a step of the kernel\n",
    "    \"\"\"\n",
    "    \n",
    "    rng = np.random.default_rng(12345) #A numpy generator \n",
    "    \n",
    "    x = np.zeros(shape = (2))\n",
    "    # Computations for u_t\n",
    "    uniu = dists.Uniform(a = 0., b = 1.).rvs(size = 1) # Uniform to \"choose a move\"\n",
    "        \n",
    "    if xp[0] == 0 :\n",
    "        if uniu <= eta:\n",
    "            # Dirac on xp = 0 \n",
    "            x[0] = 0\n",
    "        else:\n",
    "            # Exponential law with rate zeta \n",
    "            x[0] = rng.exponential(scale = 1 / zeta, size = 1)\n",
    "            \n",
    "    elif xp[0] !=0:\n",
    "        if uniu <= alpha:\n",
    "            # Dirac on xp\n",
    "            x[0] = xp[0]\n",
    "\n",
    "        elif uniu <= alpha + beta:\n",
    "            # Exponential law with rate zeta \n",
    "            x[0] = rng.exponential(scale = 1 / zeta, size = 1)\n",
    "\n",
    "        elif uniu <= alpha + beta + beta_0:\n",
    "            # Dirac on 0\n",
    "            x[0] = 0\n",
    "\n",
    "        elif uniu <= alpha + beta + beta_0 + gamma_u:\n",
    "            # Drift upward (rate lambda_u/xp[0])\n",
    "            x[0] = xp[0] + \n",
    "\n",
    "        else:\n",
    "            # Drift downward (rate lambda_d/xp[0])\n",
    "            x[0] = xp[0] - rng.exponential(scale = xp[0]/lambda_d, size = 1)\n",
    "        \n",
    "        \n",
    "        # Computations for s_t\n",
    "        unis = dists.Uniform(a = 0., b = 1.).rvs(size = 1) # Uniform to \"choose\" a move \n",
    "               \n",
    "        if unis <= alpha_s:\n",
    "            x[1] = xp[1]\n",
    "        else:\n",
    "            x[1] = dists.Gamma(a = kappa_s, b = kappa_s).rvs(size = 1))\n",
    "            \n",
    "        return x \n",
    "\n",
    "def sampleY(x, kappa, theta, epsilon_b, epsilon_0, b):\n",
    "    \"\"\"\n",
    "    Samples an observation y_t based on the vector x_t and some parameters\n",
    "    Uses the 1st expression of the emission model \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample the number of molecules initially sampled (x_t)\n",
    "    x_t = dists.Poison(rate = x[0]*x[1]/(kappa*theta).rvs(size = 1)\n",
    "                           \n",
    "    # Sample the amplificaiton coefficient (a_t)\n",
    "    a_t = dists.Gamma(kappa, 1/theta)\n",
    "                       \n",
    "    # Sample the read counts (y_t)\n",
    "    uniy = dists.Uniform(a = 0., b = 1.).rvs(size = 1) # Uniform to \"choose a move\"\n",
    "    \n",
    "    # With proba \"1 - epsilon_b - epsilon_0\"\n",
    "    if uniy < 1 - epsilon_b - epsilon_0:\n",
    "        y = dists.Poisson(rate = x_t*a_t).rvs(size = 1)\n",
    "    \n",
    "    # With proba \"epsilon_b\"\n",
    "    elif uniy < 1 - epsilon_0:\n",
    "        # Generate following a Poisson distribution truncated in 0               \n",
    "        proposition = 0 \n",
    "        while proposition == 0:\n",
    "            proposition = dists.Poisson(rate = 1*a_t).rvs(size = 1)\n",
    "        y = proposition\n",
    "                       \n",
    "    # With proba \"epsilon_0\"\n",
    "    else:\n",
    "        y = dists.DiscreteUniform(0, b+1) # In the supplementary materials, they stae that b should be equal to max(y)\n",
    "                                          # so we have to do a choice here\n",
    "    return y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65dda4",
   "metadata": {},
   "source": [
    "**I don't know what to choose as initial law, for now it is just a Dirac on (0,0)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(length, \n",
    "                  eta, alpha, zeta, beta, beta_0, gamma_u, gamma_d, lambda_u, lambda_d,\n",
    "                alpha_s, kappa_s,\n",
    "                 kappa, theta, b):\n",
    "    \"\"\"\n",
    "    Function that allows to generate data (that we can then analyze)\n",
    "    length  is the length of the data to be generated\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialisation \n",
    "    X = np.zeros(shape = (length, 2))\n",
    "    Y = np.zeros(shape = length)\n",
    "    \n",
    "    # Time = 0\n",
    "    X[0] = np.array([0,0]) \n",
    "    Y[0] = sampleY(X[0], kappa =kappa, theta = theta, epsilon_b = epsilon_b, epsilon_0 = epsilon_0, b = b)\n",
    "    \n",
    "    # Time = 1, ...... \n",
    "    for time in range(1, length):\n",
    "        # Update X\n",
    "        X[time] = stepx(X[time - 1], eta = eta, alpha = alpha, zeta = zeta, beta = beta, beta_0 = beta_0, \n",
    "                        gamma_u = gamma_u, gamma_d=gamma_d, lambda_u = lambda_u, lambda_d = lambda_d,\n",
    "                        alpha_s = alpha_s, kappa_s = kappa_s)\n",
    "        # Sample from Y|X\n",
    "        Y[time] = ssampleY(X[time], kappa =kappa, theta = theta, epsilon_b = epsilon_b, epsilon_0 = epsilon_0, b = b)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "820e8d1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2551365168.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\leo\\AppData\\Local\\Temp\\ipykernel_12824\\2551365168.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def RnaProb(particles.FeynmanKac):\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class RnaProb(particles.FeynmanKac):\n",
    "    \n",
    "    \n",
    "    def __init__(self, a = 0.): # ALl parameters should be added there\n",
    "        self.a = a # And so on for all parameters \n",
    "                 \n",
    "    def M0(self, N):\n",
    "    \n",
    "    def M(self, t, xp):\n",
    "                 \n",
    "        # Initialize empty vector\n",
    "        # It has two columns, one for u_t and one for s_t\n",
    "        x = np.zeros(shape = (N, 2)) # Maybe this N won't work\n",
    "        \n",
    "        # Loop over the particles\n",
    "        for i in range(N):\n",
    "            # Call the function that does one move using the kernel of x\n",
    "            x[i,:] = stepX(xp[i,:], \n",
    "                          eta, alpha, zeta, beta, beta_0, gamma_u, gamma_d, lambda_u, lambda_d,\n",
    "                          alpha_s, kappa_s)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "                 \n",
    "    def logG(self, t, xp, x):\n",
    "                 \n",
    "                 \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b996877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.zeros(shape = (2))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f369e37c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f9645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fada59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc7b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33bce30b",
   "metadata": {},
   "source": [
    "# Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d9373",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Should probably be transformed into a function so that it can be reused for the sampling\n",
    "                 \n",
    "            # Computations for u_t\n",
    "            uniu = dists.Uniform(a = 0., b = 1.).rvs(size = 1) # Uniform to \"choose a move\"\n",
    "        \n",
    "            if xp[i, 0] == 0 :\n",
    "                if uniu <= self.eta:\n",
    "                # Dirac on xp = 0 \n",
    "                    x[i,0] = 0\n",
    "                else:\n",
    "                    # Exponential law with rate zeta \n",
    "                    x[i,0] = dists.Gamma(1, self.zeta).rvs(size = 1)\n",
    "            \n",
    "            elif xp[i,0] !=0:\n",
    "                if uniu <= self.alpha:\n",
    "                # Dirac on xp\n",
    "                    x[i,0] = xp[i, 0]\n",
    "\n",
    "                elif uniu <= self.alpha + self.beta:\n",
    "                    # Exponential law with rate zeta \n",
    "                    x[i,0] = dists.Gamma(1, self.zeta).rvs(size = 1)\n",
    "\n",
    "                elif uniu <= self.alpha + self.beta + self.beta_0:\n",
    "                    # Dirac on 0\n",
    "                    x[i,0] = 0\n",
    "\n",
    "                elif uniu <= self.alpha + self.beta + self.beta_0 + self.gamma_u:\n",
    "                    # Drift upward\n",
    "                    x[i,0] = xp[i,0] + dists.Gamma(1, self.lambda_u/xp[i,0]).rvs(size = 1)\n",
    "\n",
    "                else:\n",
    "                    # Drift downward \n",
    "                    x[i,0] = xp[i,0] - dists.Gamma(1, self.lambda_d/xp[i,0]).rvs(size = 1)\n",
    "            \n",
    "                # Computations for s_t\n",
    "                unis = dists.Uniform(a = 0., b = 1.).rvs(size = 1) # Uniform to \"choose\" a move \n",
    "               \n",
    "                if unis <= self.alpha_s:\n",
    "                    x[i,1] = xp[i,1]\n",
    "                else:\n",
    "                    x[i,1] = dists.Gamma(a = self.kappa_s, b = self.kappa_s).rvs(size = 1)\n",
    "                 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
