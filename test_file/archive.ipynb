{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5336045e",
   "metadata": {},
   "source": [
    "# IMPORTANT\n",
    "\n",
    "Due to huge changes in the project, this notebook has been transformed into an archive file. \n",
    "See the new file main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e5522",
   "metadata": {},
   "source": [
    "# Packages installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install packages needed \n",
    "!pip install matplotlib\n",
    "!pip install particles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae8cb38",
   "metadata": {},
   "source": [
    "# Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a836eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Modules from particles\n",
    "import particles \n",
    "from particles import distributions as dists # Where proba distributions are defined\n",
    "from particles import state_space_models as ssm # Where state-space-models are defined\n",
    "from particles.collectors import Moments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1669d",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b025e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the negative binomial \n",
    "\n",
    "class NegativeBinomiale(dists.DiscreteDist):\n",
    "    \n",
    "    \"\"\"\n",
    "    Negative binomial distribution\n",
    "    Relies on scipy.stats\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.nbinom.html\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n = 1, p = 0.5):\n",
    "        self.n = n \n",
    "        self.p = p\n",
    "    \n",
    "    # Random variable sampler\n",
    "    def rvs(self, size = None):\n",
    "        return stats.nbinom(self.n, self.p, size = size)\n",
    "    \n",
    "    # Log probability density (mass) function \n",
    "    def logpdf(self, x):\n",
    "        return stats.nbinom.logpmf(x, self.n, self.p)\n",
    "    \n",
    "    # Percentile point function\n",
    "    def ppf(self, u):\n",
    "        return stats.nbinom.ppf(u, self.n, self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef74edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class expression(ssm.StateSpaceModel):\n",
    "    \n",
    "    \"\"\"\n",
    "    # Work in progress. In the end, we should add default values of the parameters here. \n",
    "    default_params = {\"eta\":, \"alpha\":, \"beta\":, \"beta_0\", \"gamma_u\":, \"gamma_d\":,\n",
    "                     \"zeta\":, \"lamba_u\":, \"lambda_d\": ,\n",
    "                     \"alpha_s\":, \"kappa_s\":,\n",
    "                     \"epsilon_b\":, \"epsilon_0\":, \"b\":}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initial distribution of the data (X0)\n",
    "    def PX0(self): \n",
    "    \n",
    "    \n",
    "    # Transition kernel \n",
    "    def PX(self, t, xp): # Distribution of X_t given X_{t-1} = xp (p = past)\n",
    "        \n",
    "        # Here, X involves two things: u_t (0th index) and s_t (1th index)\n",
    "        # Therefore, we have to define both \n",
    "        \n",
    "        # For u_t \n",
    "        uni_x = dists.Uniform(a = 0, b = 1).rvs(size = 1) # Uniform to \"choose a move\"\n",
    "        \n",
    "        if xp[:,0] == 0 :\n",
    "            if uni <= self.eta:\n",
    "                # Dirac on xp = 0 \n",
    "                u_t =  dists.Dirac(loc = 0)\n",
    "            else:\n",
    "                # Exponential law with rate zeta \n",
    "                u_t = dists.Gamma(1, zeta)\n",
    "            \n",
    "        elif xp[:,0] !=0:\n",
    "            \n",
    "            if uni <= alpha:\n",
    "                # Dirac on xp\n",
    "                u_t = dists.Dirac(loc = xp[:, 0])\n",
    "                \n",
    "            elif uni <= alpha + beta:\n",
    "                # Exponential law with rate zeta \n",
    "                u_t = dists.Gamma(1, zeta)\n",
    "            \n",
    "            elif uni <= alpha + beta + beta_0:\n",
    "                # Dirac on 0\n",
    "                u_t = dists.Dirac(loc = 0)\n",
    "            \n",
    "            elif uni <= alpha + beta + beta_0 + gamma_u:\n",
    "                # Drift upward\n",
    "                u_t = dists.linearD(dists.Gamma(1, lambda_u/xp[:,0]), a = 1, b = xp[:,0])\n",
    "            \n",
    "            else:\n",
    "                # Drift downward \n",
    "                u_t = dists.linearD(dists.InvD(dists.Gamma(1, lambda_d), a = 1, b =xp[:,0])\n",
    "                                    \n",
    "                \n",
    "        #For s_t\n",
    "        uni_s = dists.Uniform(a = 0, b = 1).rvs(size = 1) # Uniform to \"choose\" a move \n",
    "        \n",
    "        if unis <= alpha_s:\n",
    "            s_t = dists.Dirac(loc = xp[:,1])\n",
    "        else:\n",
    "            s_t = dists.Gamma(a = kappa_s, b = kappa_s)\n",
    "        \n",
    "        # Return the independant product of u_t and s_t\n",
    "        return dists.IndeProd(u_t, s_t)\n",
    "            \n",
    "    # Emission model\n",
    "    def PY(self, t, xp, x): # Distribution of Y_t given X_t = x (and possibly X_{t-1} =xp)\n",
    "        mix = dists.Mixture([1 - epsilon_b - epsilon0, epsilon_b, epsilon_0],\n",
    "                            dists.,\n",
    "                            dists.,\n",
    "                            dists.DiscreteUniform(lo = 0, hi = b+1) # \"In practice, b is set to max(y)\"\n",
    "                            )\n",
    "        return mix \n",
    "    \n",
    "                                    \n",
    "                                    \n",
    "    # We should use the first expression (the one not involving the infinite sum), but involving x_t and a_t. \n",
    "    # And so we might need conditionnal distributions ? \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682c7926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abdist(xp):  # xp means X_{t-1}\n",
    "    d = {'u': #Rewrite the big expression,\n",
    "         's': #Rewrite the simple kernel,\n",
    "         'a': dists.Gamma(kappa, 1/theta),\n",
    "         'x': dists.Cond(lambda x: dists.Poisson(x[\"u\"]*s[u]/(kappa*theta)), # Et il faudrait rajouter ces paramÃ¨tres}\n",
    "    return dists.StructDist(d)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def usdist(xp):  # xp means X_{t-1}\n",
    "    d = {'u': dists.Normal(loc=xp['a']),\n",
    "         's': dists.Cond(lambda x: dists.Dirac(xp['b'] + x['a']))}\n",
    "    return dists.StructDist(d)\n",
    "\n",
    "class SillyModel(ssms.StateSpaceModel):\n",
    "    def PX0(self):\n",
    "        return abdist({'a': 0., 'b': 0.})\n",
    "    def PX(self, t, xp):\n",
    "        return abdist(xp)\n",
    "    def PY(self, t, xp, x):\n",
    "        return dists.Normal(loc=x['a'], scale=0.3)  # whatever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa532817",
   "metadata": {},
   "source": [
    "# Older"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7204ef9",
   "metadata": {},
   "source": [
    "What we have to do \n",
    "1. **Describe the model using the pacakge particles.**\n",
    "    - This means we have to \"code\" the initial law $X_0$ (which we will chose somehow, not that important), the Markov transition kernel of $X_t$ and the emission law of $Y_t$. The two last one are described thoroughly in the **supplementary materials** of the paper, but we might have technical difficulties for implementing it. \n",
    "    - Then, we will be able to *generate* observations using the model.\n",
    "   \n",
    "2. **Implement a particle filter with a fixed $\\Theta$**\n",
    "    - That is, recoverint $X_t$ based on $Y_t$, in a situation where all the parameter values are known.\n",
    "\n",
    "3. **Implement two ways to do bayesian inference on the model.**\n",
    "    - That is, estimate the values of the parameters.\n",
    "    - **Implementation 1**: Particle Gibbs (like in the paper)\n",
    "    - **Implementation 2**: PMMH (simpler to implement but harder to calibrate). \n",
    "    - And compare those two implementations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c40bd",
   "metadata": {},
   "source": [
    "## About the distributions \n",
    "\n",
    "All probability distributions are not available in the package (see the documentation https://particles-sequential-monte-carlo-in-python.readthedocs.io/en/latest/distributions.html). However, it is possible to define **transformations** of distributions and, as such, to create new ones. The two next transformations will prove useful. \n",
    "\n",
    "If $X \\sim \\mathcal{U}(0,1)$ then $-\\frac{1}{\\lambda} log X \\sim \\mathcal{E}(\\lambda)$ (here, $\\lambda$ is the **rate**). \n",
    "\n",
    "If $Y \\sim \\mathcal{G}a(n, \\frac{1-p}{p})$ and $ X|y \\sim \\mathcal{P}(y)$ then $X \\sim \\mathcal{N}eg(n,p)$  \n",
    "\n",
    "- We can define our exponential laws more simply, as $\\mathcal{E}(\\lambda) \\sim \\mathcal{G}amma(1, \\lambda)$\n",
    "- For the Negative Binomial, we should define a new probability (using ProbDist) and interface it with scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.nbinom.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29886fd1",
   "metadata": {},
   "source": [
    "## On the use of uniforms to make \"choices\"\n",
    "\n",
    "Here, we rely on the \"particles\" package for the distributions. \n",
    "The following is the code that allows to draw a number from a uniform continuous distribution on [0,1]. \n",
    "Note that first we $define$ the law (as an object), then we can generate numbers from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d2626b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6896325]\n",
      "[0.14226246]\n"
     ]
    }
   ],
   "source": [
    "Uniform = dists.Uniform(a = 0, b = 1)\n",
    "# Generate one number from a (continuous) uniform(0,1)\n",
    "x = Uniform.rvs(size = 1)\n",
    "print(x)\n",
    "\n",
    "# Equivalently\n",
    "y = dists.Uniform(a = 0., b = 1.).rvs(size = 1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce6bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a bunch of uniforms \n",
    "vector_x = Uniform.rvs(size = 10000)\n",
    "#plt.hist(vector_x, density = True, bins = 100)\n",
    "#plt.show()\n",
    "\n",
    "# Equivalently \n",
    "vector_y = dists.Uniform(a = 0., b = 1.).rvs(size = 10000)\n",
    "#plt.hist(vector_y, density = True, bins = 100)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef58ee4b",
   "metadata": {},
   "source": [
    "In our case, uniforms have two different uses:\n",
    "- We can create more complex distributions based on uniforms (see next section for more details)\n",
    "- We can use them to make **choices**\n",
    "    \n",
    "The markov transition kernels for $u_{t}$ consists in a mixture of different possible **moves**. If, for example, $u_{t-1} = 0$, $u_t$ either stays at 0 (with probability $1 - \\eta$) or follows an exponential distribution (with probability $\\eta$). In our implementation, it is necessary, at each step, to choose between these options.\n",
    "\n",
    "One way to do it is to generate a uniform $u \\sim \\mathcal{U}(0,1)$, then to check if $ u < \\eta$. If so, $u_{t}$ should follow an exponential, if not, it should stay at 0.\n",
    "\n",
    "This procedure can be extended to multiple moves. Say we have 3 options, with respective probabilities $\\alpha$, $\\beta$ and $\\gamma$ such that $\\alpha + \\beta + \\gamma = 1$. First we generate $u \\sim \\mathcal{U}(0,1)$. Then we check:\n",
    "- $u < \\alpha$ ? If so: option 1, if not, continue the loop.\n",
    "- $u < \\alpha + \\beta$ ? If so:option 2, if not, continue the loop.\n",
    "- Given that probabilities sum up to 1, the only possibility left is option 3. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class expression(ssm.StateSpaceModel):\n",
    "    \n",
    "    \"\"\"\n",
    "    # Work in progress. In the end, we should add default values of the parameters here. \n",
    "    default_params = {\"eta\":, \"alpha\":, \"beta\":, \"beta_0\", \"gamma_u\":, \"gamma_d\":,\n",
    "                     \"zeta\":, \"lamba_u\":, \"lambda_d\": ,\n",
    "                     \"alpha_s\":, \"kappa_s\":}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initial distribution of the data (X0)\n",
    "    def PX0(self): \n",
    "    \n",
    "    \n",
    "    # Transition kernel \n",
    "    def PX(self, t, xp): # Distribution of X_t given X_{t-1} = xp (p = past)\n",
    "        # Here, X involves two things: u_t (0th index) and s_t (1th index)\n",
    "        # Therefore, we have to define both \n",
    "        \n",
    "        # For u_t \n",
    "        \n",
    "        uni_x = dists.Uniform(a = 0, b = 1).rvs(size = 1) # Uniform to \"choose a move\"\n",
    "        \n",
    "        if xp[:,0] == 0 :\n",
    "            if uni <= self.eta:\n",
    "                # Dirac on xp = 0 \n",
    "                u_t =  dists.Dirac(loc = 0)\n",
    "            else:\n",
    "                # Exponential law with rate zeta \n",
    "                u_t = dists.LinearD(dists.LogD(dists.Uniform(a = 0, b = 1)), a = - 1 / zeta)\n",
    "            \n",
    "        elif xp[:,0] !=0:\n",
    "            if uni <= alpha:\n",
    "                # Dirac on xp\n",
    "                u_t = dists.Dirac(loc = xp[:, 0])\n",
    "                \n",
    "            elif uni <= alpha + beta:\n",
    "                # Exponential law with rate zeta \n",
    "                u_t = dists.LinearD(dists.LogD(dists.Uniform(a = 0, b = 1)), a = - 1 / zeta)\n",
    "            \n",
    "            elif uni <= alpha + beta + beta_0:\n",
    "                # Dirac on 0\n",
    "                u_t = dists.Dirac(loc = 0)\n",
    "            \n",
    "            elif uni <= alpha + beta + beta_0 + gamma_u:\n",
    "                #\n",
    "                u_t =  \n",
    "            \n",
    "            else:\n",
    "                #\n",
    "                u_t = \n",
    "                \n",
    "        #For s_t\n",
    "        \n",
    "        uni_s = dists.Uniform(a = 0, b = 1).rvs(size = 1) # Uniform to \"choose\" a move \n",
    "        if uni_s <= alpha_s:\n",
    "            s_t = dists.Dirac(loc = xp[:,1])\n",
    "        else:\n",
    "            s_t = dists.Gamma(a = kappa_s, b = kappa_s)\n",
    "        \n",
    "        # Return the independant product of u_t and s_t\n",
    "        return dists.IndeProd(u_t, s_t)\n",
    "\n",
    "            \n",
    "    # Emission model\n",
    "    def PY(self, t, xp, x): # Distribution of Y_t given X_t = x (and possibly X_{t-1} =xp)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
